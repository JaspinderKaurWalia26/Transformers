# -*- coding: utf-8 -*-
"""lora_implementation_on_distilBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WNRf5nYpJY2UEhYsi9qEMHFXeR0FbgYX
"""

!pip install transformers datasets

import numpy as np
from sklearn.metrics import accuracy_score, f1_score
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import transformers
from google.colab import drive
drive.mount('/content/drive')

# loading dataset
dataset = load_dataset("ag_news")
print(dataset)
# tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
# tokenize function
def tokenize_function(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )
# applying mapping
tokenized_dataset = dataset.map(tokenize_function, batched=True)
# setting format
tokenized_dataset.set_format("torch")

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForSequenceClassification

# loading model
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=4
)

# Configure LoRA
from peft import LoraConfig

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_lin", "v_lin"],
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS"
)
# Attaching LoRA to Model
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# compute metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }

# setting training parameters
training_args = TrainingArguments(
    output_dir="/content/results",
    learning_rate=2e-4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=1,
    weight_decay=0.01,
    save_strategy="no",
    logging_strategy="steps",
    logging_steps=100
)

# initializing trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
)

# start training
trainer.train()

# evaluating
eval_results = trainer.evaluate()
print(eval_results)

# saving to drive
drive_save_path = "/content/drive/MyDrive/lora_model"
trainer.model.save_pretrained(drive_save_path)
tokenizer.save_pretrained(drive_save_path)

print(f"Model and tokenizer successfully saved to: {drive_save_path}")

# training for 2 epochs now to compare with full fine tuning
training_args = TrainingArguments(
    output_dir="/content/lora/results",
    learning_rate=2e-4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    save_strategy="no",
    logging_strategy="steps",
    logging_steps=100
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
)

trainer.train()

eval_results = trainer.evaluate()
print(eval_results)

drive_save_path = "/content/drive/MyDrive/lora_modelnew"
trainer.model.save_pretrained(drive_save_path)
tokenizer.save_pretrained(drive_save_path)

print(f"Model and tokenizer successfully saved to: {drive_save_path}")