# -*- coding: utf-8 -*-
"""tokenization_attention_exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MVd3oO6sZgDg_W8f03xoEMUK4M5npfzW
"""

# Inspecting Tokenization Internals

from transformers import AutoTokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
text = "Transformers are changing NLP forever."
tokens = tokenizer(text, return_tensors="pt")
print(tokens)

# Converting input_ids back to tokens
print(tokenizer.convert_ids_to_tokens(tokens["input_ids"][0]))

# Creating padded input manually:
tokens = tokenizer(
    text,
    padding="max_length",
    max_length=20,
    truncation=True,
    return_tensors="pt"
)

print(tokens["attention_mask"])

# Manual Forward Pass
from transformers import AutoModel
import torch

model = AutoModel.from_pretrained(
    model_name,
    output_hidden_states=True,
    output_attentions=True
)

outputs = model(**tokens)

# analyzing hidden states
hidden_states = outputs.hidden_states
print(len(hidden_states))
print(hidden_states[0].shape)

# Attention Weights Analysis
attentions = outputs.attentions
print(len(attentions))
print(attentions[0].shape)

tokens_list = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])
print(tokens_list)
import matplotlib.pyplot as plt
import seaborn as sns
layer_index = 0
head_index = 0
attention_matrix = attentions[layer_index][0, head_index, :, :]

# Plot
plt.figure(figsize=(10,8))
sns.heatmap(attention_matrix.detach().numpy(), cmap='viridis',
            xticklabels=tokens_list, yticklabels=tokens_list)
plt.xlabel("Key tokens")
plt.ylabel("Query tokens")
plt.title(f"Attention Heatmap: Layer {layer_index}, Head {head_index}")
plt.show()

"""
Task: Attention Interpretation
Take sentence:
“The animal didn’t cross the street because it was too tired.”
Find:
 Which word does “it” attend to most?
This shows coreference modeling.
Extract attention weights for the token index corresponding to “it”.
"""


from transformers import AutoTokenizer, AutoModel
import torch
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)
text = "The animal didn’t cross the street because it was too tired."
tokens = tokenizer(text, return_tensors="pt")
tokens_list = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])
print("Tokenized sentence:", tokens_list)
outputs = model(**tokens)
attentions = outputs.attentions
print("Number of layers:", len(attentions))
print("Attention matrix shape (layer 0):", attentions[0].shape)
it_index = (tokens['input_ids'] == tokenizer.convert_tokens_to_ids("it")).nonzero(as_tuple=True)[1].item()
print("Token index of 'it':", it_index)
last_layer_attention = attentions[-1]
it_attention_all_heads = last_layer_attention[0, :, it_index, :]
it_attention_mean = it_attention_all_heads.mean(dim=0)
max_index = torch.argmax(it_attention_mean).item()
print("Token 'it' attends most to:", tokens_list[max_index])

# Layer-wise Representation Evolution

from transformers import AutoTokenizer, AutoModel
import torch

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_hidden_states=True)
text = "Transformers are changing NLP forever."
tokens = tokenizer(text, return_tensors="pt")
tokens_list = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])
outputs = model(**tokens)
hidden_states = outputs.hidden_states
nlp_index = (tokens['input_ids'] == tokenizer.convert_tokens_to_ids("nl")).nonzero(as_tuple=True)[1].item()
print("Token index of 'NLP':", nlp_index)
embedding_layer = hidden_states[0][0, nlp_index, :]
middle_layer = hidden_states[6][0, nlp_index, :]
final_layer = hidden_states[-1][0, nlp_index, :]
print("Embedding layer vector:", embedding_layer[:10])
print("Middle layer vector:", middle_layer[:10])
print("Final layer vector:", final_layer[:10])

"""
Contextual Difference Experiment
Take two sentences:
1. “The bank approved the loan.”
2. “The river bank was beautiful.”
Tokenize both.
Extract final-layer embedding of “bank”.
Compute cosine similarity between the two embeddings.
"""

from transformers import AutoTokenizer, AutoModel
import torch
from torch.nn.functional import cosine_similarity

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_hidden_states=True)
sentences = [
    "The bank approved the loan.",
    "The river bank was beautiful."
]

tokens_list_all = []
tokens_all = []

for sent in sentences:
    tokens = tokenizer(sent, return_tensors="pt")
    tokens_all.append(tokens)
    tokens_list_all.append(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))

for i, t in enumerate(tokens_list_all):
    print(f"Sentence {i+1} tokens:", t)



final_embeddings = []

for i, tokens in enumerate(tokens_all):
    outputs = model(**tokens)
    hidden_states = outputs.hidden_states
    tokens_list = tokens_list_all[i]
    bank_index = tokens_list.index("bank")
    final_layer = hidden_states[-1][0, bank_index, :]
    final_embeddings.append(final_layer)

print("Embeddings extracted for 'bank' in both sentences.")

similarity = cosine_similarity(final_embeddings[0], final_embeddings[1], dim=0)
print("Cosine similarity between 'bank' embeddings:", similarity.item())