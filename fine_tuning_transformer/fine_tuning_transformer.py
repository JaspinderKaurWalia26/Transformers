# -*- coding: utf-8 -*-
"""fine_tuning_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n8J9pBY-e3LuJ7nD8AIpFGGMGavszetb
"""

!pip install transformers datasets

import numpy as np
from sklearn.metrics import accuracy_score, f1_score
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import transformers
from google.colab import drive

# mount the drive
drive.mount('/content/drive')

# loading dataset
dataset = load_dataset("ag_news")
print(dataset)


tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# tokenize function
def tokenize_function(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

# applying mapping
tokenized_dataset = dataset.map(tokenize_function, batched=True)
# setting data format
tokenized_dataset.set_format("torch")

# defining model
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=4
)

# setting training arguments
training_args = TrainingArguments(
    output_dir="/content/results",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    save_strategy="no",
    logging_strategy="steps",
    logging_steps=100
)

# compute metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }

# Initializing trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
)

# fine tuning start
trainer.train()

# evaluating
trainer.evaluate()
# drive save path
drive_save_path = "/content/drive/MyDrive/fine_tuned_model"
# saving to drive
trainer.model.save_pretrained(drive_save_path)
tokenizer.save_pretrained(drive_save_path)

print(f"Model and tokenizer successfully saved to: {drive_save_path}")

# loading from drive
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer
import numpy as np
from datasets import load_dataset
from sklearn.metrics import accuracy_score, f1_score
from google.colab import drive
drive.mount('/content/drive')
drive_save_path = "/content/drive/MyDrive/fine_tuned_model"

tokenizer = AutoTokenizer.from_pretrained(drive_save_path)
model = AutoModelForSequenceClassification.from_pretrained(drive_save_path)

# loading tokenizer
dataset = load_dataset("ag_news")

# tokenize function
def tokenize_function(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format("torch")

# compute metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }

# temporary trainer for evaluation
trainer = Trainer(
    model=model,
    compute_metrics=compute_metrics
)

model = AutoModelForSequenceClassification.from_pretrained("/content/drive/MyDrive/fine_tuned_model")

train_metrics = trainer.evaluate(tokenized_dataset["train"])
print("Training Metrics:", train_metrics)
val_metrics = trainer.evaluate(tokenized_dataset["test"])
print("Validation Metrics:", val_metrics)

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
model_path = "/content/drive/MyDrive/fine_tuned_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.eval()
text = "Apple is releasing new products this fall."

inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

with torch.no_grad():
    outputs = model(**inputs)


logits = outputs.logits
predicted_class_id = torch.argmax(logits, dim=1).item()


labels = ["World", "Sports", "Business", "Sci/Tech"]
predicted_label = labels[predicted_class_id]

print(f"Sentence: {text}")
print(f"Predicted label: {predicted_label}")