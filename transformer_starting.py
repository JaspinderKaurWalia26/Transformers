# -*- coding: utf-8 -*-
"""Transformer_Starting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ABOOWa1BYdSNkzcg_FRbhxNxVd8rva2i

Practical Section (Day 1 Implementation)
"""

! pip install transformers datasets torch

#Task 1: Load Pretrained Transformer for Classification
#First use pipeline:


from transformers import pipeline
classifier = pipeline("sentiment-analysis")
print(classifier("Transformers are powerful models."))

#Now loading manually:

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "distilbert-base-uncased-finetuned-sst-2-english"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

text = "Transformers are powerful models."
inputs = tokenizer(text, return_tensors="pt")

outputs = model(**inputs)
logits = outputs.logits

prediction = torch.argmax(logits, dim=1)
print(prediction)

#Task 2: Inspect Hidden States
#Enable hidden states:

from transformers import AutoTokenizer, AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    output_hidden_states=True
)

outputs = model(**inputs)
hidden_states = outputs.hidden_states

print(len(hidden_states))  # number of layers
print(hidden_states[0].shape)

from transformers.utils import logging
logging.set_verbosity_error()

from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

model_name = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(
    model_name,
    output_hidden_states=True
)

sentence1 = "The bank approved the loan."
sentence2 = "The river bank is beautiful."

inputs1 = tokenizer(sentence1, return_tensors="pt")
inputs2 = tokenizer(sentence2, return_tensors="pt")

outputs1 = model(**inputs1)
outputs2 = model(**inputs2)


hidden1 = outputs1.hidden_states[-1]
hidden2 = outputs2.hidden_states[-1]


tokens1 = tokenizer.tokenize(sentence1)
tokens2 = tokenizer.tokenize(sentence2)

bank_index_1 = tokens1.index("bank")
bank_index_2 = tokens2.index("bank")


bank_embedding_1 = hidden1[0, bank_index_1]
bank_embedding_2 = hidden2[0, bank_index_2]


similarity = F.cosine_similarity(
    bank_embedding_1,
    bank_embedding_2,
    dim=0
)

print("Sentence 1 tokens:", tokens1)
print("Sentence 2 tokens:", tokens2)
print("Cosine similarity between 'bank' embeddings:", similarity.item())